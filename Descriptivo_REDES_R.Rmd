---
title: "Proyecto"
author: "Samuel Lozano Gómez"
date: "15 de mayo de 2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(fpp2)
library(forecast)
library(patchwork)
library(slider)
library(seasonal)
library(ggplot2)
library(grid)
library(ggplotify)
library(stats)
library(knitr)
library(RSNNS)
library(zoo)
library(quantmod)
library(nnfor)
library(keras)
library(tensorflow)
#install.packages(c("dplyr", "readxl", "fpp2", "forecast", "patchwork", "slider", "seasonal", 
#                  "grid", "ggplotify", "status", "knitr", "RSNNS", "quantmod", "nnfor", "keras"))
```

Cargamos los datos
```{r}
df = read_xlsx("Mallorca.xlsx")
```

Creamos los dos objetos ts() y los mostramos en función del tiempo:
```{r}
tem = ts(df$tempm, start = c(1,1), frequency=24)
dem = ts(df$demanda, start = c(1,1), frequency=24)
plot(tem)
plot(dem)
```
En primer lugar estudiaremos la serie principal, de la demanda eléctrica:

Vamos a observar las componentes de la serie de tendencia, estacionalidad, componente aleatoria varianza. Podemos ver los correlogramas o la descomposición STL:
```{r}
acf(dem, lag.max = 28)
pacf(dem, lag.max = 28)
stl_dem<-stl(dem, s.window = "per")
autoplot(stl_dem)
```
Vemos un decrecimiento lento hasta 0, por lo que sabemos de la existencia de tendencia, además estacionalidad de periodo 24, es decir, diaria, pues el coeficiente 24 es significativo y elevado. Además, al observar la descomposición STL parece que hay más estacionalidades. Probaremos con una msts, con estacionalidades diaria y semanal, aplicando una mstl.

```{r}
x <- msts(dem, seasonal.periods=c(24,7*24), start=c(1,1))
stl_x<-mstl(x, s.window = "per")
autoplot(stl_x)
```
Esta vez está mejor, vemos que la serie claramente tenía dos estacionalidades.

Respecto a la varianza observamos ya en la serie que no es constante pues el rango cada vez se va haciendo más grande. Mostramos la demada y el logaritmo de la demanda

```{r}
plot(dem)
ldem<-log(dem)
plot(ldem)
```

En resumen, la serie de Demanda eléctrica presenta Tendencia, dos estacionalidades (diaria y semanal) y varianza no constante. 

Veamos ahora qué sucede con la serie de la temperatura:
```{r}
acf(tem, lag.max = 28)
pacf(tem, lag.max = 28)
stl_tem<-stl(tem, s.window = "per")
autoplot(stl_tem)
```
Vemos hechos similares a la demanda: lento decrecimiento hacia 0 en la ACF con estacionalidad diaria. Puede que encontremos una segunda estacionalidad, miraramos pues la msts con estacionalidad semanal.
```{r}
y <- msts(tem, seasonal.periods=c(24,7*24), start=c(1,1))
stl_y<-mstl(y, s.window = "per")
autoplot(stl_y)
```
De nuevo un acierto, pues presenta doble estacionalidad. Veamos la variaza:
```{r}
plot(tem)
ltem<-log(tem)
plot(ltem)
```
Es difícil asegurarlo pues a simple vista no lo parece. Lo tendremos en cuenta posteriormente.


Veamos ahora las dos series al mismo tiempo.

Descomposición STL
```{r}
x <- msts(dem, seasonal.periods=c(24,7*24))
stl_x<-mstl(x, s.window = "per")
autoplot(stl_x)
y <- msts(tem, seasonal.periods=c(24,7*24))
stl_y<-mstl(y, s.window = "per")
autoplot(stl_y)
```

Vamos a mostrarlas en la misma escala.
```{r}
par(mfrow = c(1,1))
ts.plot(scale(df[0:250,]), col = c(1, 2), lwd = c(1, 2))
title("Total Annual Recreational Visitors")
legend("topleft", c("Temperatura", "Demanda"), lty = c(1,1,1), col = c(1:2), lwd = 2)
```

Parece que guardan una alta correlación, pero vamos a verlo matemáticamente
```{r}
cor.test(df$tempm, df$demanda)
```

Efectivamente, correlación de 0.826 y p-valor inferior a 0.05. Con esto sabemos que hay correlación, pero, ¿hay causalidad?
```{r}
diff.zt<-diff(df$demanda)
diff.xt<-diff(df$tempm)
cor.test(diff.zt,diff.xt)
```

Vemos que hay una leve causalidad, significativa con p-valor menor que 0.05. Con esto podemos tratar de predecir a demanda utilzando la temperatura como variable exógena.

Empezaremos por proponer modelos ARIMA que nos permitan entender el comportamiento de la serie de demanda, para luego crear redes neuronales con los mismos parámetros.

Dado lo visto anteiormente, realizamos un modelo tentativo ARIMA(1,1,1)x(1,1,1)
```{r}
a = arima(log(dem), order=c(1,1,1), seasonal=c(1,1,1))

#par(mfrow=c(1,2))
acf(ts(a$residuals), lag.max=28,main="residuos modelo ajustado",ylim=c(-1,1))
pacf(ts(a$residuals), lag.max=28,main=" residuos modelo ajustado ",ylim=c(-1,1))
cpgram(a$residuals,main="peridiograma acumulado de los residuos")
layout(matrix(c(1,1,2,3),2,2, byrow=TRUE))
plot(a$residuals,main='Residuos del modelo ARIMA ajustado')
hist(a$residuals, br=12, main='Residuos')
qqnorm(a$residuals, main='Residuos')
qqline(a$residuals, main='Residuos')
```
Tras varios intentos nos decantamos por un ARIMA(1,1,1)x(1,1,1), ya que vemos que conseguimos captar buena parte de la información, y los residuos se asemejan a un ruido blanco.
Realicemos predicciones. Vamos a predecir las últimas 168 horas, esto es los últimos 7 días, dejando 54 para el entrenamiento. Computaremos el MAE y el RMSE:
```{r}
#train = window(x, start=c(1,1), end=c(8,120))
#test = window(x, start=c(8,121))
train = window(dem, end=c(54,24))
test = window(dem, start=c(55,1))

#train = window(y, start=c(1,1), end=c(8,120))
#test = window(y, start=c(8,121))
train.tem = window(tem, end=c(54,24))
test.tem = window(tem, start=c(55,1))

a = arima(log(train), order=c(1,1,1), seasonal=c(1,1,1))
pred <- forecast(a, 168)
pred$mean = exp(pred$mean)
MAE <- mean(abs(pred$mean - test))
RMSE <- sqrt(mean((pred$mean - test)^2))

a2 = arima(log(train), order=c(1,1,1), seasonal=c(1,1,1), xreg=log(train.tem))
pred2 <- forecast(a, test.tem)
pred2$mean = exp(pred2$mean)
MAE2 <- mean(abs(pred2$mean - test))
RMSE2 <- sqrt(mean((pred2$mean - test)^2))

kable(data.frame(Modelo=c("ARIMA", "ARIMA_xreg"),MAE=round(c(MAE, MAE2),4),RMSE=round(c(RMSE, RMSE2),4)))
```
Con varibale exógena los resultados son ampliamente mejores.

El siguiente paso será aplicar distintos modelos de redes neuronales a fin de conseguir buenas predicciones comparando los enfoques de utilzar la temperatura como variable exógnea y también sin utilizarla.


```{r}
train = window(log(dem), end=c(54,24))
test = window(dem, start=c(55,1))

train.tem = window(log(tem), end=c(54,24))
test.tem = window(log(tem), start=c(55,1))
```

NARX
```{r}
narx_fit <- nnetar(train, lambda="auto")
narx_pred<-forecast(narx_fit,168)
narx_pred$mean<-exp(narx_pred$mean)
narx_MAE <- mean(abs(narx_pred$mean - test))
narx_RMSE <- sqrt(mean((narx_pred$mean - test)^2))

narx2_fit <- nnetar(train, lambda="auto", xreg=train.tem)
narx2_pred<-forecast(narx2_fit,xreg=test.tem)
narx2_pred$mean<-exp(narx2_pred$mean)
narx2_MAE <- mean(abs(narx2_pred$mean - test))
narx2_RMSE <- sqrt(mean((narx2_pred$mean - test)^2))


kable(data.frame(Modelo=c("NARX", "NARX_xreg"), Tipo=c(narx_fit$method, narx2_fit$method),MAE=round(c(narx_MAE, narx2_MAE),4),RMSE=round(c(narx_RMSE, narx2_RMSE),4)))
```

Elman
```{r}
demx<-(dem-min(dem))/(max(dem)-min(dem))

t <- 1:1296
tst = 1297:length(dem)

y<-as.zoo(demx)
x1<-Lag(y,k=1)
x2<-Lag(y,k=2)
x3<-Lag(y,k=3)
x4<-Lag(y,k=4)
x5<-Lag(y,k=5)
x6<-Lag(y,k=6)
x7<-Lag(y,k=7)
x8<-Lag(y,k=8)
x9<-Lag(y,k=9)
x10<-Lag(y,k=10)
x11<-Lag(y,k=11)
x12<-Lag(y,k=12)
x13<-Lag(y,k=13)
x14<-Lag(y,k=14)
x15<-Lag(y,k=15)
x16<-Lag(y,k=16)
x17<-Lag(y,k=17)
x18<-Lag(y,k=18)
x19<-Lag(y,k=19)
x20<-Lag(y,k=20)
x21<-Lag(y,k=21)
x22<-Lag(y,k=22)
x23<-Lag(y,k=23)
x24<-Lag(y,k=24)
slog<-cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,
            x16,x17,x18,x19,x20,x21,x22,x23,x24)
slog<-slog[-(1:24),]
inputs<-slog[,2:25]
outputs<-slog[,1]

elman_fit <- elman(inputs[t],outputs[t], size=c(24,7),learnFuncParams=c(0.1), maxit=100)
y <- as.vector(outputs[-t])
e_y = y*(max(dem)-min(dem))+min(dem)
elman_pred <- predict(elman_fit, inputs[-t])

e_pred = elman_pred*(max(dem)-min(dem))+min(dem)

elman_MAE <- mean(abs(e_pred - e_y))
elman_RMSE <- sqrt(mean((e_pred - e_y)^2))
```

Jordan (Al ejecutarlo muestra un error y necesita reiniciar el entorno)
```{r}
#adfasfas
#jordan_fit<-jordan(inputs[t],
#    outputs[t],
#    size=c(24,7),
#    learnFuncParams=c(0.01),
#    maxit=100)
#
#y <- as.vector(outputs[-t])
#j_y = y*(max(dem)-min(dem))+min(dem)
#jordan_pred <- predict(jordan_fit, inputs[-t])
#
#j_pred = jordan_pred*(max(dem)-min(dem))+min(dem)
#
#jordan_MAE <- mean(abs(j_pred - j_y))
#jordan_RMSE <- sqrt(mean((j_pred - j_y)^2))
```


MLP
```{r}
mlp_fit = mlp(train, rep=20)
mlp_pred = forecast(mlp_fit, h=168)
mlp_pred$mean = exp(mlp_pred$mean)
mpl_MAE <- mean(abs(mlp_pred$mean - test))
mlp_RMSE <- sqrt(mean((mlp_pred$mean - test)^2))
```

LSTM
```{r}
s_train = window(dem, end=c(40,24))
s_val = window(dem, start=c(41,1), end=c(54,24))
s_test = test

model = keras_model_sequential() %>%
  layer_lstm(units = 7, return_sequences = TRUE, input_shape = c(7, 1)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)

model %>% compile(
  loss = 'mean_squared_error',
  optimizer = 'adam'
)

model %>% fit(
  s_train,
  epochs = 3,
  batch_size = 1,
  validation_data = s_val
)

# Evaluate the model
#evaluation_result <- model %>% evaluate(x_test, y_test)

# Print evaluation metrics
#print(evaluation_result)

# Visualize predictions vs. actual values
lstm_pred <- model %>% predict(x_test)
plot(y_test, type = "l", col = "blue", ylim = range(c(y_test, lstm_pred)), 
     xlab = "Time", ylab = "Value", main = "LSTM Model Evaluation")
lines(predictions, col = "red")
legend("topleft", legend = c("Actual", "Predicted"), col = c("blue", "red"), lty = 1)

```




Pasos:
- Ver características de la serie (Tendencia, estacionalidades, Varianza)
- Listar modelos NN para probar:
  - 
  - 
- Modelos:
  - A la serie demanda sin variarla
  - A la serie demanda sin variarla + Xt temperatura
  - A la serie demanda transforamda
  - A la serie demanda transforamda + Xt temperatura
- Elegir mejor enfoque / modelos y compararlos en gráficos
-Diseñar la APP



